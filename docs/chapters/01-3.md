---
title: 问题动机、线性代数与视觉化
translation-date:
translator: Titus Tzeng
---


## 资源

请[在推特上跟随 Alfredo Canziani @alfcnz](https://twitter.com/alfcnz)。影片、讲述线性代数与奇异值分解（SVD）相关细节的课本都可以在 Alfredo 的推特上找到，比如在搜索栏输入`linear algebra (from:alfcnz)`。


## [变换与其动机](https://www.youtube.com/watch?v=5_qrxVq1kvc&t=233s)
让我们看看一个图像分类的例子。比方我们用一个一百万像素的相机拍一张照片，这张图片铅直、水平各有约 1000 个像素，而每个像素会有红绿蓝三个色彩的维度。每个图像因此就可以当作三百万维空间中的一点。在这样大的维度中，许多我们想分类的有趣图像——像是狗对上猫——必然会在空间中的相同区域中。

为了有效分离这谢图像，我们考虑对资料进行一些变换以移动这些资料点。请回想一下，在二维空间中，一个线性变换等同于矩阵乘法。例如下列这些线性变换：

-   旋转（当矩阵为正交矩阵）
-   缩放（当矩阵为对角矩阵）
-   反射（当行列式为负值）
-   错切

值得注意的是，平移不是一个线性变换，因为原点不会保留在原处，但它是仿射变换。再回到我们的图像例子，我们可以先平移使各点移动到 0 的周遭，再用对角矩阵缩放来放大那个区域，最后，我们就能寻找能分割空间将各点分到各自的类别的直线。亦即，藉由线性与非线性的转换，将资料点映射到一个使他们线性可分的空间。下个部份我们会更具体的说明这个想法。


## [资料视觉化 － 通过神经网路将不同颜色的点分离](https://www.youtube.com/watch?v=5_qrxVq1kvc&t=798s)

我们的视觉化展现的是由五个股组成的螺旋，每个股对应不同颜色。这些点在一个二维平面中，可以用元组来代表；这些颜色代表第三个维度，或者当作是各点的类别。接着我们可以用一个网路来分开不同颜色的点。

| <center><img src="Spiral1.png" width="200px"/></center> | <center><img src="Spiral2.png" width="200px"/></center> |
|                 (a) 输入各点，通过网路前                  |                 (b) 输出各点，通过网路后                  |

<center> 图 1: 五色的螺旋 </center>
这个网路会拉伸空间以使各点能被分离. 当收敛时，网路会把每种颜色分开至最终流形中的不同子空间。也就是说，新的空间中的每个颜色，对于一对全的回归都是线性可分的。<!--The vectors in the diagram can be represented by a five by two matrix;-->图中的这些向量可以用一个 5 x 2 的矩阵表示<!---->；这个矩阵乘上每个点产出属于五种不同颜色的分数，于是每个点可以再以它们的分数分为不同颜色。这里输出维度是五，每个颜色有一个；输入维度则是二，对应 x 与 y 坐标。总之，这个网路对空间进行变换，而这种变换可参数化为数个矩阵与非线性变换。


### 网路结构

<center>
<img src="Network.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
图 2: 网路结构
</center>

The first matrix maps the two dimensional input to a 100 dimensional intermediate hidden layer. We then have a non-linear layer, `ReLU` or Rectified Linear Unit, which is simply *positive part* $(\cdot)^+$ function. Next, to display our image in a graphical representation, we include an embedding layer that maps the 100 dimensional hidden layer input to a two-dimensional output. Lastly, the embedding layer is projected to the final, five-dimensional layer of the network, representing a score for each colour.### Network architecture

<center>
<img src="Network.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
Figure 2: Network Architecture
</center>

The first matrix maps the two dimensional input to a 100 dimensional intermediate hidden layer. We then have a non-linear layer, `ReLU` or Rectified Linear Unit, which is simply *positive part* $(\cdot)^+$ function. Next, to display our image in a graphical representation, we include an embedding layer that maps the 100 dimensional hidden layer input to a two-dimensional output. Lastly, the embedding layer is projected to the final, five-dimensional layer of the network, representing a score for each colour.
